#Linear Regression To Calculate Profits For Food Truck
```{r eval = F}
require(devtools)
install_github('rCharts', 'ramnathv')
rCharts::open_notebook()
```

Linear Regression Approach To Predict Profits To a Food Truck

We implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.

We would like to use this data to help you select which city to expand to next.

##Input Dataset:

The file ex1data1.txt contains the dataset for our linear regression problem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss.
```{r eval = F}
require(devtools)
install_github('rCharts', 'ramnathv')
rCharts::open_notebook()
```
```{r echo = T, message = F, cache = F}
library(googleVis)
op <- options(gvis.plot.tag='chart')
require(rCharts)
options(RCHART_WIDTH = 300, RCHART_HEIGHT = 200)
knitr::opts_chunk$set(comment = NA, results = 'asis', tidy = F, message = F)
```
<style>.rChart {width: 100px; height: 100px;}</style>

## Linear Regression Using rChart

```{r, message=F, warning=F}
library(googleVis)
library(rCharts)
library(ggplot2)
```

```{r, echo=FALSE, eval=TRUE}
truckData<-read.csv('ex1data1.txt',header=FALSE)
colnames(truckData)<-c('Population','Profit')
dat = fortify(lm(Population~Profit, data = truckData))
names(dat) = gsub('\\.', '_', names(dat))
```

```{r gvisScatterChart, results='asis'}
Scatter1 <- gvisScatterChart(truckData,options=list(legend="none",title="Food Truck", vAxis="{title:'Profit in $10,000s'}",hAxis="{title:'Population of City in 10,000s'}", width=300, height=300))
plot(Scatter1)
```


```{r chart2, results='asis'}
p1 <- rPlot(Population ~ Profit, data = dat, type='point')
p1$layer(y = '_fitted', copy_layer = T, type = 'line',
  color = list(const = 'red'),
  tooltip = "function(item){return item._fitted}")
p1$print('chart2')
```

##Ordinary least squares estimation

In order to determine the optimal estimates of α and β, an estimation method known as ordinary least squares (OLS) was used. In OLS regression, the slope and intercept are chosen such that they minimize the sum of the squared errors, that is, the vertical distance between the predicted y value and the actual y value. These are the values for our residuals and are illustrated for several points in the following diagram:

![alt text](http://resources.esri.com/help/9.3/arcgisengine/java/gp_toolref/spatial_statistics_tools/regression_h.png "Logo Title Text 1")

The following is the values for residuals alpha and beta.
```{r, echo=TRUE, eval=TRUE}

beta=cov(truckData$Population,truckData$Profit)
alpha=mean(truckData$Population) - beta*mean(truckData$Profit)
beta
alpha

```

### Correlations

Using the Pearson's correlations formula we hereby calculate the correlations for the Population Vs Profit Predictibility of our food trucks.
```{r, echo=TRUE, eval=TRUE}

r=cov(truckData$Population,truckData$Profit)/(sd(truckData$Population)*sd(truckData$Profit))
r
```
Since the correlation is about ~0.84, this implies that there is a fairly strong positive linear association between the Population Vs Profit Predictibility of our food trucks. The positive association means an increase in population is correlated with the Profits for food truck in that particular city.


##Regression Analysis Using Gradient Descent:

In this part, you will fit the linear regression parameters θ to our dataset
using gradient descent.

### Update Equations/LMS:

The objective of linear regression is to minimize the cost function
![alt text](http://www.bindichen.co.uk/uploads/cost_function.png "Logo Title Text 1")

where the hypothesis hθ(x) is given by the linear model
h(x) = theta0 + theta1 * x

In batch gradient descent, each iteration performs the update 
![alt text](http://www.bindichen.co.uk/uploads/gradient_descent.png "Logo Title Text 1")



```{r, echo=TRUE, eval=TRUE}
X<-cbind(1,dat[,1])
Y<-dat[,2]
# learning rate and iteration limit
alpha <- 0.01
num_iters <- 1000
theta <- matrix(c(0,0), nrow=2)
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
```

```{r, echo=TRUE, eval=TRUE}
# squared error cost function
cost <- function(X, y, theta) {
  sum( (X %*% theta - y)^2 ) / (2*length(y))
}
# gradient descent
for (i in 1:num_iters) {
  error <- (X %*% theta - Y)
  delta <- t(X) %*% error / length(Y)
  theta <- theta - alpha * delta
  cost_history[i] <- cost(X, Y, theta)
  theta_history[[i]] <- theta
}

```


